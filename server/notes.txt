

const performTask = (items, numToProcess, processItem) => {
  let pos = 0;
  // This is run once for every numToProcess items.
  const iteration = () => {
      // Calculate last position.
      let j = Math.min(pos + numToProcess, items.length);
      // Start at current position and loop to last position.
      for (let i = pos; i < j; i++) {
          processItem(items, i);
      }
      // Increment current position.
      pos += numToProcess;
      // Only continue if there are more items to process.
      if (pos < items.length)
          setTimeout(iteration, 10); // Wait 10 ms to let the UI update.
  }
  iteration();
}

link == "helpingsurvivors.org" ||
        link == "https://helpingsurvivors.org" ||
        link == "brownandcrouppen.com" ||
        link == "https://brownandcrouppen.com" ||
        link == "cutterlaw.com" ||
        link == "https://cutterlaw.com" ||
        link == "https://lanierlawfirm.com" ||
        link == "lanierlawfirm.com" ||
        link == "nursinghomeabuse.org" ||
        link == "https://nursinghomeabuse.org" ||
        link == "socialmediavictims.org" ||
        link == "https://socialmediavictims.org" ||
        link == "m-n-law.com" ||
        link == "https://m-n-law.com" ||
        link == "samndan.com" ||
        link == "https://samndan.com" ||
        link == "omaralawgroup.org" ||
        link == "https://omaralawgroup.org" ||
        link == "https://veternsguide.org" ||
        link == "veternsguide.org" ||
        link == "https://steinlawoffices.com" ||
        link == "steinlawoffices.com" ||
        link == "https://levinperconti.com" ||
        link == "levinperconti.com" ||
        link == "https://cordiscosaile.com" ||
        link == "cordiscosaile.com" ||
        link == "https://advologix.com" ||
        link == "advologix.com" ||
        link == "https://wvpersonalinjury.com" ||
        link == "wvpersonalinjury.com" ||
        link == "nstlaw.com" || 
        link == "https://nstlaw.com"  
  
  // Step : Called to get a free proxy
  const proxyGenerator = () => {
    // Establishing the variables
    let ip_addresses = [];
    let port_numbers = [];
    let random_number;

    // Uses request and cheerio to pull the free proxies from the link and randomzies the port and proxy so when you crawl it doesn't hit the same website from the same ip address
    request("https://sslproxies.org/", function (error, response, html) {
      if (!error && response.statusCode == 200) {
        const $ = cheerio.load(html);

        $("td:nth-child(5)").each(function (index, value) {
          if ($(this).text().includes("elite")) {
            $("td:nth-child(1)").each(function (index, value) {
              ip_addresses[index] = $(this).text();
            });

            $("td:nth-child(2)").each(function (index, value) {
              port_numbers[index] = $(this).text();
            });
          }
        });
      } else {
        console.log("Error loading proxy, please try again", error);
      }

      ip_addresses.join(", ");
      port_numbers.join(", ");

      random_number = Math.floor(Math.random() * 8);

      proxyRotation = `http://${ip_addresses[random_number]}:${port_numbers[random_number]}`;
      proxyHost = ip_addresses[random_number];
      proxyPort = port_numbers[random_number];

      const proxyCheck = {
        host: ip_addresses[random_number],
        port: port_numbers[random_number],
        // proxyAuth: "y0adXjeO:pAzAHCr4",
      };

      console.log("Checking Proxy", proxyCheck);

      proxy_checker(proxyCheck)
        .then((res) => {
          console.log("Good Proxy", res); // true
          // setTimeout(function () {
          // console.log(csvLinks);
          // csvLinks.forEach((link) => {
          //   crawlerInstance.queue({
          //     uri: link,
          //     proxy: proxyRotation,
          //   });
          // });
          // }, 0);
          crawlerInstance.queue(csvLinks);
        })
        .catch((error) => {
          console.error("error", error); // ECONNRESET
          proxyGenerator();
        });
    });
  };

// For crawling sitemaps... Need to work on it
        if (crawlingURL.includes("/sitemap.xml")) {
          // https://www.apple.com/sitemap.xml
          const sitemap = $("loc");
          for (let link in sitemap) {
            if (sitemap.hasOwnProperty(link)) {
              // let logs = link + sitemap[link];
              let logs2 = sitemap[link].children;
              sitemapList.push(logs2);
              let results = sitemapList.filter((element) => {
                return element !== undefined;
              });
              results.forEach((link, index) => {
                let newLink = link[0];
                if (newLink == undefined) {
                  console.log("newLink removed");
                }
                console.log(newLink);
                FinalSitemapList.push(newLink);
                FinalSitemapList = [...new Set(FinalSitemapList)];
                FinalSitemapList = FinalSitemapList.filter((element) => {
                  return element !== undefined;
                });
              });
            }
          }
        }

// Not needed
const getListFiles = (req, res) => {
  const directoryPath = __basedir + "/resources/static/assets/uploads/";

  fs.readdir(directoryPath, function (err, files) {
    if (err) {
      res.status(500).send({
        message: "Unable to scan files!",
      });
    }

    let fileInfos = [];

    files.forEach((file) => {
      fileInfos.push({
        name: file,
        url: baseUrl + file,
      });
    });

    res.status(200).send(fileInfos);
  });
};

// Not needed
const download = (req, res) => {
  const fileName = req.params.name;
  const directoryPath = __basedir + "/resources/static/assets/uploads/";

  res.download(directoryPath + fileName, fileName, (err) => {
    if (err) {
      res.status(500).send({
        message: "Could not download the file. " + err,
      });
    }
  });
};